---
layout: archive
title: "CV"
permalink: /cv/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

Education
======
* M.S. in Data Science, École Polytechnique Fédérale de Lausanne (EPFL), 2021-2024
  * Thesis (6.0/6.0): "[HyperINF: Scaling-up Accurate Approximation of Influence Function by the Hyperpower Method](https://icml.cc/virtual/2024/36436)"
  * Advisor: [Prof. Martin Jaggi](https://people.epfl.ch/martin.jaggi)
  * Selective Courses:
    * Deep Learning (5.75/6.0)
    * Graph Theory (5.5/6.0)
    * Linear Models (5.75/6.0)
    * Time Series (5.25/6.0)
    * Applied Data Analysis (5.5/6.0)
    * Machine Learning (5.25/6.0)
    * Foundations of Data Science (5.5/6.0)
* B.S. in Computer Science and Applied Mathematics, University of Electronic Science and Technology of China, 2017-2021
  * Thesis: "Non-Parametric Bayesian Optimization"
  * Major GPA: 4.0/4.0

Research experiences
======
* *Feb. 2024 - Jun. 2024*. **Master Thesis** (Machine Learning and Optimization Lab – EPFL)
  * We propose **HyperINF** as an accurate approximation method based on a *hyperpower method*, i.e. Schulz's iterative algorithm (which enjoys a rigorous convergence guarantee) and Generalized Fisher Information Matrix (GFIM). 
  * We demonstrate the superior accuracy and stability of *HyperINF* on matrix inversion through a synthetic convergence test.
  * We further validate the efficacy of **HyperINF** through extensive real-world data attribution problems, including mislabeled data detection, data selection for LLM finetuning, and multimodal instruct-tuning data selection for VLM pretraining.

* *Jun. 2023 - Jun. 20244*. **Remote Research Assistant** (HKUST & Mila)
  * We propose **LoGAH**, with an improved low-rank decoder, that is more scalable and can predict parameters of large networks without copying while having fewer trainable parameters and a lower training cost.
  * We create a new dataset of small ViT and GPT-2 architectures, allowing GHNs to be trained on Transformers for both vision and language domains. **LoGAH** shows excellent generalized capability on larger models.
  * We outperform GHN-3 as an initialization approach in multiple vision and language tasks by predicting more diverse and performant parameters.

* *Oct.2023 - Feb.2024*. **Research Assistant** (NLP Lab – EPFL)
  * Main goal: interpret the multi-modal models including ViLT, CLIP, and BLIP.
  * We try different methods to understand how the image interacts with the text, such as the Second-Gradient, Cross-Attention map,...


* *Jul. 2022 – Dec. 2022*. **Research Assistant** (Machine Learning and Optimization Lab – EPFL)
  * We propose a two-stage model **SimSum** for document-to-document simplification tasks, combining text simplification and summarization tasks innovatively.
  * We analyse and pre-process two document-level simplification datasets, and make the resulting datasets available for reproducibility.
  * Paper was accepted to **ACL 2023** main conference.


Industry experiences
======

* *Feb.2023 - Aug.2023*. **NLP Research Intern** (AXA Group Operation Switzerland)
  * Main Task: Assess of large language models and its reasoning capabilities.
  * I Explore prompts for ChatGPT to generate different insurance claims for model's performance testing.
  * I Deploy two Fake-Text-Detection models (MPU and DetectGPT) on Synthetic Text Detection subtask.
  
  
Academic Services
======
* Reviewer for EMNLP 2025
* Reviewer for Efficient Systems for Foundation Models (ES-FoMo) Workshop, ICML 2025
* Reviewer for Data-centric Machine Learning Research (DMLR) Workshop, ICML 2024

Technical Skills
======
* **Programming Languages**: Python, C++, MATLAB
* **Machine Learning**: PyTorch, HuggingFace
* **Language Proficiency**: GRE: 328, IELTS: 7.5